# SWE-Bench-Evaluation
Evaluation of LLMs on SWE-bench via API integration. Focused on generating patches to fix GitHub issues with failing test cases while preserving existing functionality. Demonstrates real-world software engineering problem-solving

The SWE-bench Lite dataset is like a big collection of real software bugs.

For each bug (we call it a â€œtaskâ€), the dataset gives you:
	1.	Problem statement â†’ a short description of whatâ€™s broken.
	2.	Code context â†’ the piece of code where the bug happens.
	3.	Patch â†’ the actual fix that the real developer wrote (the answer).
	4.	Test patch â†’ sometimes, new tests the developer added to make sure the bug wonâ€™t come back.

So when you see "patch" in the dataset, it means:
ğŸ‘‰ â€œHere is the exact code change that fixed this bug in real life.â€

Your job (or the AIâ€™s job) is to try to come up with the same kind of fix, and then you can compare it to the datasetâ€™s patch to check if the AI solved it correctly.



cloen the repo 
git clone https://github.com/princeton-nlp/SWE-bench.git
cd SWE-bench


I'm trying with groq ai 
Iâ€™m using this llama-3.3-70b-versatile
* 
 i creared swe-bench dataset py file to check swe bench lite dataset has been loaded or not 

 next i am working on groq agent 

 import os, time, json, logging
from groq import Groq
from typing import Dict, Any, List


	â€¢	os â†’ for environment variables (API keys)
	â€¢	time â†’ used for rate limiting requests
	â€¢	json â†’ formatting output into JSON
	â€¢	logging â†’ keeps track of progress and errors
	â€¢	Groq â†’ Groq API client
	â€¢	typing â†’ type hints for cleaner code

The GroqSWEAgent class

class GroqSWEAgent:

This class acts as your agent for running SWE-bench problems on Groqâ€™s LLMs.

there are three .py files 

groq_ping.py - to check and ping Groq API
qwen_ping.py - to check and ping Qwen API
sonnet_ping.py - to check and ping Sonnet API

DATASET.PY - File contactins Swe bench lite dataset